{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iTgZA2Qa7BT",
        "outputId": "79c04751-8e33-47e7-d098-23903820cb03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for sst contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/sst\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "sst_dataset=load_dataset(\"sst\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMUfzTPrcDdF"
      },
      "source": [
        "getting training data sentences and labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jawjEvv0bfQN"
      },
      "outputs": [],
      "source": [
        "train_data = sst_dataset[\"train\"]\n",
        "sentences=train_data[\"sentence\"]\n",
        "scores= train_data[\"label\"]\n",
        "sentences = [s.lower() for s in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYAcNK2KdSCo"
      },
      "source": [
        "classify sentences according to score\n",
        "\n",
        "•From 0 to 0.2 (0.2 included) will be class 0 “very negative”.\n",
        "•From 0.2 to 0.4 (0.4 included) will be class 1 “negative”.\n",
        "•From 0.4 to 0.6 (0.6included)will be class 2 “neutral”.•From 0.6to 0.8 (0.8 included)will be class 3 “positive”.•From 0.8to 1.0(1.0included)will be class 4 “very positive”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-eOnxtKdnpi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "classes = [[] for _ in range(5)]\n",
        "#put each sentence in the class that it belongs to\n",
        "i=0\n",
        "for score in scores:\n",
        "    if score <= 0.2:\n",
        "      scores[i]=0\n",
        "      classes[0].append(sentences[i])\n",
        "    elif score > 0.2 and score <= 0.4:\n",
        "      scores[i]=1\n",
        "      classes[1].append(sentences[i])\n",
        "    elif score > 0.4 and score <=0.6:\n",
        "      scores[i]=2\n",
        "      classes[2].append(sentences[i])\n",
        "    elif score > 0.6 and score <=0.8:\n",
        "      scores[i]=3\n",
        "      classes[3].append(sentences[i])\n",
        "    else:\n",
        "      scores[i]=4\n",
        "      classes[4].append(sentences[i])\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "count number of occurences of all words in the document\n"
      ],
      "metadata": {
        "id": "hrTrcVdeD1f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "sentences_np=np.array(sentences)\n",
        "all_words = ' '.join(sentences_np).split()\n",
        "word_counts = Counter(all_words)\n",
        "word_counts_dict = dict(word_counts)\n",
        "del word_counts\n",
        "del sentences_np\n",
        "del all_words\n",
        "print(word_counts_dict['rock'])\n",
        "print(len(word_counts_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3lx4hUbDzPR",
        "outputId": "99caa793-33cb-430f-d246-19bd7558b1ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n",
            "16579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYHWmUdZdpQY"
      },
      "source": [
        "naivee bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raD4zKpydoVm"
      },
      "outputs": [],
      "source": [
        "def countWord(goal,words):#count words in a single class\n",
        "    return  np.sum(words == goal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJWGDH25jvQi"
      },
      "source": [
        "training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0d7LpD7jxiz",
        "outputId": "36318f59-26a5-48ff-8722-6cc8b4895ce0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from math import log2\n",
        "words_probablites={0: {},#will store the likelihood of each word in each class\n",
        "                  1: {},\n",
        "                   2: {},\n",
        "                   3: {},4: {}}\n",
        "Ndoc = sum(len(row) for row in classes)# counts the total number of words\n",
        "priors=[]\n",
        "\n",
        "for k in range(len(classes)):\n",
        "  Nc=len(classes[k])#count total number of words in each class\n",
        "  myclass=np.array(classes[k])\n",
        "  words = ' '.join(myclass).split()\n",
        "  word_counts = Counter(words)\n",
        "  this_word_counts_dict = dict(word_counts)#stores words count in each class to measure likelihood\n",
        "  priors.append(log2(Nc/Ndoc))\n",
        "  for word in this_word_counts_dict:\n",
        "      if word not in words_probablites[k] :#and len(word) > 1\n",
        "        #occurence=countWord(word,words)\n",
        "        log_likelihood=np.float16(log2((this_word_counts_dict[word]+1)/(word_counts_dict[word]+1)))\n",
        "        words_probablites[k][word]=log_likelihood\n",
        "words_probablites[4]['rock']\n",
        "this_word_counts_dict['rock']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test naivee bayes\n"
      ],
      "metadata": {
        "id": "tTSgkt1_tpA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_sum= [[] for _ in range(5)] #will contian the probability of each sentence to be in each class\n",
        "j=0\n",
        "for i in range (len(classes)):\n",
        "  sum=priors[i]\n",
        "  for sentence in sentences:\n",
        "    words=sentence.split(' ')\n",
        "    for word in words:#measures the probabily of each sentence for a specific class\n",
        "      #if len(word) > 1:#skip one letter words\n",
        "        if word in words_probablites[i]:#check if word appeared before in this class\n",
        "          sum+=words_probablites[i][word]\n",
        "    final_sum[i].append(sum)\n",
        "    j+=1\n",
        "\n",
        "\n",
        "max_indices = np.argmax(final_sum, axis=0)\n",
        "print(len(max_indices))\n",
        "max_indices[0]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ce5VOeJtYzK",
        "outputId": "e16a3c34-cff1-47a4-ce19-5b5795f42b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8544\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18RW4EohFh9O"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "sklearn naivee bayes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl-Lz--iFg8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e281e76-4e56-42cd-b584-0832aff59b94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 4, 2, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "model=make_pipeline(CountVectorizer(),MultinomialNB(alpha=1))\n",
        "model.fit(sentences,scores)\n",
        "labels = model.predict(sentences)\n",
        "label[0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}